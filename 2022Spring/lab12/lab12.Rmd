---
title: "Lab 12"
author: "STATC 131A"
date: "April 18, 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Welcome to the lab 12! In this lab, we will use linear regression to predict the red wine quality using physicochemical tests scores such as citric acid, pH, etc.

But first, a review of using the predict() function.

```{r}
x1 = rnorm(100)
x2 = rnorm(100)
y= 2*x1 + x2 + rnorm(100)
lm_out = lm(y~x1 + x2)
summary(lm_out)
```

Calculate prediction for y when x1 is 1 and x2 is 0.5.

```{r}
lm_out$coefficients[1] + lm_out$coefficients[2]* 1 + lm_out$coefficients[3]* 0.5 
```

Another way to do this.

```{r}
predict(lm_out, newdata = data.frame(x1= 1, x2= 0.5))
```

Can do several at once.

```{r}
predict(lm_out, newdata = data.frame(x1= c(1, 2), x2= c(0.5, -1) ))
```

We can find intervals for confidence of average or prediction interval for an individual outcome.

```{r}
predict(lm_out, newdata = data.frame(x1= c(1, 2), x2= c(0.5, -1) ), interval = "confidence")
```
```{r}
predict(lm_out, newdata = data.frame(x1= c(1, 2), x2= c(0.5, -1) ), interval = "prediction")
```

A few other notes on regression. 

1) If you try to predict one variable and include a perfectly correlated variable in the prediction set, then that variable will be perfectly fit to the outcome to the exclusion of all others.

```{r}
perf_cor = y/4
summary(lm( y ~ perf_cor + x1 + x2 ))
```


2) If there are more variables used for prediction than there are observations, `lm` will only keep the first n-1 variables.

```{r}
x3= rnorm(100)
x4= rnorm(100)
x5= rnorm(100)

all_x = data.frame(x1,x2,x3,x4,x5, y) # 100 x 6 df
lm(y~ . ,data= all_x[1:4,]) # only use the first 4 observations (4<5)
# Then lm only use the first 4-1=3 variables
```




## Wine data

The wine dataset is related to red variants of the Portuguese "Vinho Verde" wine. There are 1599 samples available in the dataset. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). 

The explanatory variables are all continuous variables based on physicochemical tests:

- **fixed acidity**
- **volatile acidity**
- **citric acid**
- **residual sugar**
- **chlorides**
- **free sulfur dioxide**
- **total sulfur dioxide**
- **density**
- **pH**
- **sulphates**
- **alcohol**

The response variable is the **quality** score between 0 and 10 (based on sensory data).

Read data. We randomly split the data into two parts-the `wine` dataset with 1199 samples and the `wine.test` dataset with 400 samples. Splitting the dataset is a common technique when we want to evaluate the model performance. There are training set, validation set, and test set. The validation set is used for model selection. That is, to estimate the performance of the different model in order to choose the best one. The test set is used for estimating the performance of our final model.

```{r}
set.seed("2022")
wine.dataset <- read.csv("winequality-red.csv", sep = ";")
test.samples <- sample(1:nrow(wine.dataset), 400)
wine <- wine.dataset[-test.samples, ]
wine.test <- wine.dataset[test.samples, ]
```

To check the correlation between explanatory variables:

```{r}
library(pheatmap)
corr.wine <- cor(wine[, -1])
pheatmap(corr.wine, treeheight_row = 0, treeheight_col = 0, 
         display_numbers = T, fontsize_number = 0.5)
```

Great! The correlations are not as high as the diamond dataset we saw in the last lab, which means we do not need to worry too much about heteroscedasticity. We now fit the linear regression using all of the explanatory variables:

```{r}
wine.fit <- lm(quality ~. ,data = na.omit(wine))
summary(wine.fit)
```


### Exercise 1 Confidence Interval


(a) Calculate the confidence interval for all the coefficients from the regression done above. Which of these factors will positively influence the wine quality?

```{r}
# Insert your code here to calculate the confidence intervals for the regression coefficients.

```

(b) Calculate the confidence intervals for the samples in `wine.test` using the model you just fit. Which confidence interval will you use? Confidence intervals for the average response or the prediction interval?

```{r}
# insert your code here and save your confidence intervals as `wine.confint`
# wine.confint <- 
```

(c) What is the percentage that your interval in (b) covers the true **quality** score in `wine.test`? What if you use the other confidence interval? Which one is consistent with your confidence level?

```{r}
# insert your code here and save your percentage as `pct.covered`
# pct.covered <- 
# pct.covered
# insert your code here and save your percentage calculated 
# using the other confidence interval as `pct.covered.other`
# wine.confint.other <- 
# pct.covered.other <- 
# pct.covered.other
```


### Exercise 2 Bootstrap CI

Scale the columns of the dataset using scale() and then make 95% bootstrap confidence intervals for the coefficients for the predictors. Plot these confidence intervals using the plotCI() function in gplots. Code from professor for making bootstrap CI is included. You can use this or write your own code. Use the wine subset as used above.

```{r}
bootstrapLM <- function(y,x, repetitions, confidence.level=0.95){
    # calculate the observed statistics
  stat.obs <- coef(lm(y~., data=x))
  # calculate the bootstrapped statistics
  bootFun<-function(){
          sampled <- sample(1:length(y), size=length(y),replace = TRUE)
          coef(lm(y[sampled]~.,data=x[sampled,])) #small correction here to make it for a matrix x
  }  
  stat.boot<-replicate(repetitions,bootFun())
  # nm <-deparse(substitute(x))
  # row.names(stat.boot)[2]<-nm
  level<-1-confidence.level
  confidence.interval <- apply(stat.boot,1,quantile,probs=c(level/2,1-level/2))
    return(list(confidence.interval = cbind("lower"=confidence.interval[1,],"estimate"=stat.obs,"upper"=confidence.interval[2,]), bootStats=stat.boot))
}
```

```{r}
# insert your code here


```

# Regression dianosis

## Red wine dataset

Reload the data.
```{r}
wine<- read.csv("winequality-red.csv", sep = ";")
wine$quality <- wine$quality + rnorm(length(wine$quality))
```

Fit the model.
```{r}
wine.fit <- lm(quality~volatile.acidity+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+pH+sulphates+alcohol, data=wine)
summary(wine.fit)
```

### Exercise 3

(a) Do regression diagnostics using the `plot` function.

```{r}
# insert your code here to do regression diagnostics.

```

(b) Answer the following TRUE/FALSE questions based on the diagnostics plot. Uncomment your answer.

```{r}
### I. The plot indicates heteroscedasticity.
# TRUE
# FALSE
### II. There are non-linearity between the explantory variable and response variable.
# TRUE
# FALSE
### III. The normal assumtion holds for this model.
# TRUE
# FALSE
```

(c) Identify at least two outliers from the data. 

```{r}

```

> I think the sample ??? and ??? are outliers.


## Diamond dataset

Read the data.

```{r}
diamonds <- read.csv("diamonds.csv")
diamonds <- diamonds[sample(1:nrow(diamonds), 1000), ]
head(diamonds)
```

Fit a linear regression.

```{r}
diamond.fit <- lm(price ~ carat + cut + color + clarity + depth + table, data = diamonds)
summary(diamond.fit)
```

### Exercise 4

(a) Do regression diagnostics using the `plot` function.

```{r}
# insert your code here to do regression diagnostics.

```

(b) Answer the following TRUE/FALSE questions based on the diagnostics plot. Uncomment your answer.

```{r}
### I. The plot indicates heteroscedasticity.
# TRUE
# FALSE
### II. There are non-linearity between the explantory variable and response variable.
# TRUE
# FALSE
### III. The normal assumtion holds for this model.
# TRUE
# FALSE
```

## Multiple regression with continuous and categorical variables

### Exercise 5

(a) Fit a linear regression model with explanatory variable `carat`, `depth`, `table`, `clarity`, `color` and `cut`.

```{r}
levels(diamonds$clarity)
levels(diamonds$color)
levels(diamonds$cut)
```


```{r}
# Insert you code here, save your model as `fit.categorical`
# fit.categorical <- 


```

(b) Write the equation when 
	i. Clarity is VS2, color is H, and cut is Premium. Replace ??? with numerical values.
		$$\text{price} = ??? + ??? \cdot \text{carat} + ??? \cdot \text{depth} + ??? \cdot \text{table}$$
	ii. clarity is I1, color is D and cut is Fair. Replace ??? by numerical values.
		$$\text{price} = ??? + ??? \cdot \text{carat} + ??? \cdot \text{depth} + ??? \cdot \text{table}$$
		
		
