---
title: "LAB 2"
author: "STAT 131A"
date: ""
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```


Welcome to the lab 2! In this lab, you will

1) review probability; 
2) review normal distributions 
3) Obtain some basic statistics by groups;



We will continue to use the rent price dataset from the lab 1. In the table **craigslist.csv**, each posting record (row) contains the following information:

- time: posting time
- price: apartment/housing monthly rent price
- size: apartment/housing size (ft^2)
- brs: number of bedrooms
- title: posting title
- link: posting link, add "https://sfbay.craigslist.org" to visit the posting page
- location: cities

Read in data. Create `one.bedrooms` data frame of only postings for 1 bedroom.

```{r ReadData}
craigslist <- read.csv("craigslist.csv", 
                       header = TRUE, stringsAsFactors = FALSE)
#Check to see if everything looks as expected:
head(craigslist)


one.bedrooms <- craigslist[craigslist$brs == 1,]
```


# Probability Distributions

There are a large number of standard parametric distributions available in R (nearly every common distribution!). To get a list of them, you can do:

```{r}
?Distributions
```

You are probably only familiar with the normal distribution. But each of them is immensely useful in statistics. You will see Chi-squared distribution, student-t distribution later in this course. Every distribution has four functions associated with it.

|Name | Explanation |
|-----|-------------|
| d | density: Probability Density Functions (pdfs) |
| p | probability: Cumulative Distribution Functions (cdfs) |
| q | quantile: the inverse of Probability Density Functions (pdfs) |
| r | random: Generate random numbers from the distribution |

**Density (pdf)** Take normal distribution $N(\mu, \sigma^2)$ for example. Let's use the `dnorm` function to calculate the density of $N(1, 2)$ at $x = 0$:

```{r}
dnorm(0, mean = 1, sd = sqrt(2))
# Notice here the parameter in dnorm is sd,
# which represents the standard deviation (sigma instead of sigma^2). 
```

Looks familiar? It is exactly same the function that you wrote in lab 0! This function can accept vectors and calculate their densities as well. 

**Cummulative Distribution Functions (cdf)** The Cummulative Distribution Functions (cdfs) gives you the probability that the random variable is less than or equal to value:
$$P(X \leq z)$$
where $z$ is a constant and $X$ is the random variable. For example, in the above example. $N(1, 2)$ is symmetric about 1. Then what would be the probability that it is less than 1?

```{r}
probs <- pnorm(1, mean = 1, sd = sqrt(2))
probs
```

**Quantiles** `qnorm` is the inverse function of `pnorm`. It accepts value $p$ (probability) from 0 to 1, and returns a value $q$ (quantile) satisfies the following condition:

$$P(X \leq q) = p$$
where $X$ is the random variable. A toy example would be:

```{r}
qtl <- qnorm(0.5, mean = 1, sd = sqrt(2))
qtl
```

Quantiles are very important in hypothesis testing, for example if you want to know what value of a test-statistic would give you a certain p-value. 

**Random number generation** The last function is very important when you simulate stuff. We use it to generate numbers from a distribution. 

```{r}
# generate 1000 samples from Normal(1, 2) distribution.
samples <- rnorm(n= 1000, mean = 1, sd = sqrt(2))
# plot the histogram
hist(samples)

```



```{r}
class(craigslist)
class(samples)
sample_df = data.frame(sample = samples)
ggplot(sample_df, aes(x = samples)) +
  geom_histogram(bins = 10, 
                 fill = 'grey', # fill in color
                 color = 'black') + # border color
  theme_bw() +
  labs(title = 'Histogram of samples')
```

## Plotting a function

Let's plot the line of this function (you can also use the `curve` function that the professor used for the lecture notes to draw functions):

We will be manually recreating what `curve` does: we will create a sequence of x values and then evaluate f(x) to get the corresponding y vector of values, and then plot them with `plot`

First we create the sequences

```{r}
# create a vector, spaced by 0.1
x <- seq(from = -4, to = 6, by = 0.1)
# calculate their densities
x.dens <- dnorm(x, mean = 1, sd = 2)
```

Now we will plot them

```{r}
# plot the line
plot(x, x.dens, ylab="Density, p(x)")
```

```{r}
x_df = data.frame(x = x, x.dens = x.dens)
ggplot(x_df, aes(x, x.dens)) +
  geom_point(shape = 1) +
  labs(y = 'Density, p(x)') +
  theme_bw()
```

We'd really rather plot a curve, than these points. We can do this by setting `type="l"`; this makes `plot` act like the function `lines`, with the difference that it doesn't have to add to an existing plot.

```{r}
# plot the line
plot(x, x.dens,type="l", ylab="Density, p(x)",xlim=c(-4,6))
```

```{r}
ggplot(x_df, aes(x, x.dens)) +
  geom_line() +
  labs(y = 'Density, p(x)') +
  theme_bw()
```

It's useful to know how to manually plot function as described above, but for simple functions `curve` can be clearer

```{r}
# plot the line
curve(dnorm(x, mean = 1, sd = 2),from=-4,to=6, 
      ylab="Density, p(x)")
```

We can also overlay the density on top of the histogram, but we need to make sure the histogram is on the density scale:

```{r}
hist(samples, freq=FALSE,xlim=c(-4,6))
curve(dnorm(x, mean = 1, sd = 2),
      ylab="Density, p(x)", add=TRUE)

```

```{r}
ggplot(sample_df, aes(samples, y = ..density..)) +
  geom_histogram(bins = 10, fill = 'grey', color = 'black') +
  geom_line(aes(x = x, y = x.dens),data = x_df) +
  labs(y = 'Density, p(x)', title = 'Histogram of samples') +
  theme_bw()
```


\pagebreak

**Exercise 1.**

(a) $X_1$ follows Normal Distribution $N(3.5, 9)$. What is the probability that $-2.5 < X \leq 9.5$? 

```{r e1a}
pnorm(9.5, mean = 3.5, sd = 3) - pnorm(-2.5, mean = 3, sd = 3)

```

(b) $X_2$ follows Normal Distribution $N(3.5, 9)$. Theoretically, what is the expected value of the interquartile range (IQR) if we plot samples from $X_2$? (HINT: IQR = 0.75 quantile - 0.25 quantile.)

```{r e1b}
# insert code here save 


```

\pagebreak

# Set Seed

The random numbers and random samples generated in R are produced by a random number generator. The process is not really "random", but mimic the results of what we would get from the random process. Thus, unlike a truly random process, the random numbers can be tracked and be exactly reproduce. For example, if you run a permutation test function for two times, you would get two very close but different p-values. But if you set the seed to be the same number before you run the permutation test, you would obtain the exact same p-values. Throughout the rest of the course where random number generation is involved, we would set seed of the random number generator such that the results are fully reproducible (important for grading purposes!). However, in the real application, you would generally change it. 

The following chunk illustrate how `set.seed` influence the random number generation. 

```{r}
set.seed(201728)
sample(x = 1:5, size = 3) # after calling this function, the seed will be updated
sample(x = 1:5, size = 3) # the seed has changed thus we would get a different number
set.seed(201728) # set the seed back to 201728
sample(x = 1:5, size = 3)
```

It is the same with `rnorm`:

```{r}
set.seed(20170126)
rnorm(5, mean = 0, sd = 1)
rnorm(5, mean = 0, sd = 1)
set.seed(20170126)
rnorm(5, mean = 0, sd = 1)
```


\pagebreak

# Replicate

`replicate` function in R can be used to replicate a function a certain number of times. The replicate function takes two arguments -- the number of replications (B), and the function to replicate. 

We can use the replicate function along with the sample function to simulate rolls of a dice.

```{r}

dice <- c(1,2,3,4,5,6)  ## Setting up the die

sample(dice,1) ### A single roll of the draw

B = 10 ##Number of rolls

replicate(B, sample(dice,1))  ## Getting the outcomes for 10 rolls of a die

```



**Exercise ** 

Find the mean and sd of house prices in the craigslist dataset

```{r e2a}
#insert code here

```

Draw a sample of size 50 from craigslist price column and compute the mean of the sample

```{r e2b}
#insert code here


```

Repeat the process of drawing a sample of size 50 and calculating it's mean 100 times using `replicate` function 

```{r e2c}
#insert code here

#Mean100Draws = 

```

Draw a density histogram of 100 replicated draws

```{r e2d}
#insert code here


```

Add a normal curve to the density histogram (using the mean and sd from part a )

```{r e2e}


```




\pagebreak

# Summarize dataset by groups

In this dataset, we are more interested in the summaries of rent price by cities. The `tapply` function is useful when we need to break a vector into groups (subvectors), and apply a function (for example, the `mean` function) within each group. The usage is:

```{}
tapply(Variable_of_interest, Factor_vector_representing_groups, Function_you_want_to_apply_on)
```

For example, to obtain the median rent price by cities.

```{r}
tapply(craigslist$price, craigslist$location, median)
```

You can write and apply your own functions. For example, to get the percentage of rent price less than $2000/month by city.

```{r}
tapply(craigslist$price, craigslist$location, function(x){mean(x < 2000)})
```

The rent price in Berkeley is much better than Palo Alto! The median monthly rent is much lower. And the percentage of rent price less than $2000 per month is much higher. But do not rush to conclusions, let us break down the dataset further. 

\pagebreak

**Exercise 3** 

Use `tapply` to get following statistics for each city.

(a) The percentage of listings that are one bedroom;

```{r e3a}
# insert code here save the precentage of one bedrooms by cites as
# 'pct.1b'
```

(b) the median price of one bedroom listings. (Use the subset `one.bedrooms` created above)

```{r e3b}
# insert code here save the median of one bedrooms by cites as
# 'med.ib'
```

\pagebreak

There are more one-bedroom rent postings in Berkeley. The median prices of one-bedrooms are less different for Berkeley and Palo Alto. The fact that the overall median price differs may be caused by the large proportion of small apartment postings in Berkeley. How you obtain the sample may greatly influence your results. Lets look at a stratified sampling dataset, where houses/apartments with 1, 2, 3 bedrooms account for 40\%, 40\%, 20\% of the total samples of each city.

```{r}
prop = c(0.4, 0.4, 0.2)
samples = c()
for (city in unique(craigslist$location)){
  for (b in 1:3){
    samples = c(samples, sample(which(craigslist$brs == b & craigslist$location == city), prop[b]*60))
  }
}
craigslist.srs <- craigslist[samples, ]
```

Now we look at the median rent price by cities for the stratified sampling dataset.

```{r}
tapply(craigslist.srs$price, craigslist.srs$location, median)
```

Below is the percentage of rent price less than $2000/month by cities for the stratified sampling dataset.

```{r}
tapply(craigslist.srs$price, craigslist.srs$location, function(x){mean(x < 2000)})
```

Now the difference of price median between Berkeley and Palo Alto is reduced compared to the SRS sample. This is a case where simple random samples may be misleading. The results from stratified samples may well depend on how you assign the proportions to each stratum. Care must be taken before you reach conclusions.


